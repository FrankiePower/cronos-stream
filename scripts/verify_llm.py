
import asyncio
import os
import sys
from dotenv import load_dotenv

# Ensure we can import from the a2a-service module
sys.path.append(os.path.join(os.getcwd(), 'a2a', 'a2a-service'))

# Importing the actual classes from the project
try:
    from host.lib.openai.client import OpenAIClient
    from host.lib.openai.planner import PaywallPlanner
except ImportError as e:
    print(f"Error importing modules: {e}")
    print("Make sure you are running this script from the project root.")
    sys.exit(1)

async def test_llm_engine():
    print("--- Verifying LLM Engine Integration ---")
    
    # 1. Load Environment
    load_dotenv("a2a/a2a-service/.env")
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("‚ùå Error: OPENAI_API_KEY not found in a2a/a2a-service/.env")
        return

    print(f"‚úÖ Found OpenAI Key (starts with {api_key[:5]}...)")

    # 2. Initialize Client
    try:
        client = OpenAIClient(model="gpt-3.5-turbo") # Use cheaper model for test
        planner = PaywallPlanner(client)
        print("‚úÖ OpenAIClient & PaywallPlanner initialized")
    except Exception as e:
        print(f"‚ùå Failed to initialize client: {e}")
        return

    # 3. Test LLM Call
    print("\n--- Sending Test Prompt to LLM ---")
    user_request = "I need to analyze some financial data."
    print(f"User Request: '{user_request}'")
    
    try:
        # We manually call the logic that PaywallPlanner uses, or just use the client directly to prove access
        # Let's use the planner to see if it picks a tool/agent
        # Note: PaywallPlanner.plan() expects a specific format usually, let's look at its signature if this fails.
        # But for 'actually uses the engine', a simple chat completion is the gold standard.
        
        response = await client.chat([
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "Say 'LLM Engine Verify Success' if you can hear me."}
        ])
        
        # client.chat returns a string directly
        print(f"\nü§ñ LLM Response: \"{response}\"")
        
        if "Success" in response:
             print("\n‚úÖ VERIFICATION PASSED: The code is actively communicating with OpenAI.")
        else:
             print("\n‚ö†Ô∏è  Response received but keyword missing. Still checking connection... OK.")

    except Exception as e:
         print(f"‚ùå LLM Call Failed: {e}")

if __name__ == "__main__":
    asyncio.run(test_llm_engine())
